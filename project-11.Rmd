---
title: "Predicting AirBnB Rental Rates"
author: "Trevor Isaacson, Jonathan Olavarria, Jasmine DeMeyer"
date: "12/10/2021"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE, echo = FALSE}
set.seed(445)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5, fig.height=3) 
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(e1071)
library(rmarkdown)
library(glmnet)
library(knitr)
library(leaps)
library(tree)
library(dplyr)
library(caret)
library(gbm)
library(randomForest)
library(GGally)
library(pls)
library(splines)
library(boot)
```

```{r}
original_df = read.csv("training_data.csv")
nrow(original_df)
head(original_df)
```

probably don't want the individual id of each place, its website url, description, amenities, name, zipcode (we have lat/long and city) and converted log_price to price
```{r}
training_df = original_df %>% 
  mutate(price = log_price)  %>%
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X, log_price)) %>%
  mutate(first_review = as.Date(original_df$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(original_df$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(original_df$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", original_df$host_response_rate))/100)
```

```{r}
head(training_df)
```

```{r}
# update 3 date columns to track the start of the week instead of the individual date
# create 3 new columns to track dates in the form of month instead of individual date
training_df = training_df %>%
  mutate(first_review_year = as.Date(cut(training_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(training_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(training_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

training_df = training_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

training_df = training_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee))

training_df = training_df[complete.cases(training_df), ]
```

```{r}
# these appear in training set and not testing set so I took them out
training_df <- training_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Island") %>%
  filter(property_type != "Tent") %>%
  filter(property_type != "Treehouse") %>%
  filter(property_type != "Yurt") %>%
  filter(property_type != "Hut") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  select(-c(first_review, last_review, host_since))

head(training_df)
```

# Variable Descriptions:
## Price
```{r}
ggplot(data = training_df) +
  geom_histogram(aes(price), bins = 25) +
  ggtitle("Listing Price ($)") + 
  geom_vline(xintercept = median(training_df$price), color = "blue", lty = 2)

```

## Property Type
```{r}
training_df %>%
  group_by(property_type) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(room_type) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()
```

## City
```{r}
training_df %>%
  group_by(city) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total))

# latitude and longitude
ggplot(data = training_df) +
  geom_point(aes(x = longitude, y = latitude, color = city)) +
  ggtitle("Latitude and Longitude")

```



## Property Details
```{r}
training_df %>%
  group_by(accommodates) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(bedrooms) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(bathrooms) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(bed_type) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

```

## Host details
```{r}
training_df %>%
  group_by(host_has_profile_pic) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(host_identity_verified) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(instant_bookable) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(cancellation_policy) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) 

training_df %>%
  group_by(host_response_rate) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()
```


## Variable Selection?
Should we first choose which variables should be included in everybody's models or have everybody make their two models and then variable select based on the best model they found? 

Correlation Plot?
```{r}
head(training_df)
# Original df
ggcorr(original_df)
```






## Models
Do we want to split the training_data in half and make one half training and the other half available to use for CV testing and then the testing_data.csv is used for the final model predictions since it doesn't have a price column?
```{r}
trn = sample(seq_len(nrow(training_df)), 9000)
training = training_df[trn, ]
testing = training_df[-trn, ]
```

Baseline: Linear Regression
```{r}
linear = lm(price ~ ., data = training)
summary(linear)
```

```{r}
linear_predict = predict(linear, newdata = testing, type = "response")
MSE_testing = (testing$price - linear_predict)^2
print(paste("MSE of Testing Set: ", mean(MSE_testing)))
```

Leave One Out Cross Validation on Linear Regression Model:
```{r warning = FALSE}
n = 107
results_LOOCV = c()
for(i in seq_len(n)){
  trn <- seq_len(n) != i
  
  #fit model
  l = lm(price ~ ., data = training_df[trn, ])
  
  # predict on validation set
  pred = predict(l, training_df[!trn, ])
  
  # estimate test MSE
  true_price = training_df[!trn, ]$price
  
  results_LOOCV[i] = (true_price - pred)^2
}

print(paste("Leave One Out Cross Validation: ", round(mean(results_LOOCV), 2)))
```

k-Fold Cross Validation on Linear Regression Model:
```{r warning = FALSE}
k = 10
folds = sample(seq_len(k), nrow(training_df) , replace = TRUE)
results_kfold = c()
for(i in seq_len(k)){
  trn <- folds != i
  
  #fit model
  l = lm(price ~ ., data = training_df[trn, ])
  
  # predict on validation set
  pred = predict(l, training_df[!trn, ])
  
  # estimate test MSE
  true_price = training_df[!trn, ]$price
  
  results_kfold[i] = (true_price - pred)^2
}

linearReg_kfold_MSE = round(mean(results_kfold), 2)
print(paste("k-fold Cross Validation: ", linearReg_kfold_MSE))
```


JJ: PCR and PLS

```{r}
#Fit PCR
m0 <- pcr(price~., data = training,ncomp= 20, validation = "CV")
mse <- MSEP(m0)
data.frame(M = mse$comps, mse = t(as.data.frame(mse$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse))
# based off of plot we will select 15 components (elbow method) = 99.97% of variance explained
mse$val
summary(m0)

# R^2 vs Principal Components Plot
validationplot(m0, val.type = "R2")

# Test Set Predictions
pcr_pred <- predict(m0, testing, ncomp = 15)

# Prediction Plot
predplot(m0, ncomp = 15)

# Check for NA values
sum(is.na(pcr_pred))
sum(is.na(testing$price))

# Calculate Test Set MSE
PCR_mse <- round(mean((testing$price - pcr_pred)^2), 4)
PCR_mse


```


```{r}
# Fit PLS
m1 <- plsr(price~., data=training, validation="CV",ncomp=20)

# Plot MSE
mse1 <- MSEP(m1)
data.frame(M = mse1$comps, mse = t(as.data.frame(mse1$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse))

# Number of Components chosen is 10 = 99.93% of Variance Explained
summary(m1)

# Test Set Predictions
pls_pred <- predict(m1, testing, ncomp = 10)

# Prediction Plot
predplot(m1, ncomp = 10)

# Check for NA values
sum(is.na(pls_pred))

# Calculate Test Set MSE
PLS_mse <- round(mean((testing$price - pls_pred)^2), 4)
PLS_mse

```


Jasmine: Regression Splines/Generalized Additive Models
```{r}
set.seed(445)
head(training_df)
ggpairs(training_df, columns = c(2:8, 19))
ggpairs(training_df, columns = c(9:14, 19))
ggpairs(training_df, columns = 15:22)
head(training)

#spline on accommodates
fit0 <- glm(price ~ bs(accommodates, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(price ~ bs(accommodates, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(accommodates, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(accommodates, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_accomodates <- data.frame(degfree, cv)
df_accomodates

#spline on review_scores_rating
fit <- glm(price ~ bs(review_scores_rating, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(review_scores_rating, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(review_scores_rating, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_reviews <- data.frame(degfree, cv)
df_reviews

#spline on bathrooms
fit <- glm(price ~ bs(bathrooms, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(bathrooms, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(bathrooms, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_bathrooms <- data.frame(degfree, cv)
df_bathrooms

#spline on bedrooms
fit <- glm(price ~ bs(bedrooms, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(bedrooms, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

degfree <- c(3, 4)
cv <- c(cvs, cvs2)
df_bedrooms <- data.frame(degfree, cv)
df_bedrooms
```

Best spline was on accommodates with 4 degrees of freedom

```{r warning = FALSE}
#fit GAM
gammod <- lm(price ~ . + bs(accommodates, df = 2) + bs(review_scores_rating, df = 4) + bs(bathrooms, df = 4) + bs(bedrooms, df = 3), data = training)
summary(gammod)

gam_predict = predict(gammod, newdata = testing)
gam_MSE = round(mean((testing$price - gam_predict)^2), 4)
print(paste("Test MSE of GAM: ", gam_MSE))
```
GAM was my best performing model, but overall didn't perform super well. Probably not our best model.




Trevor: Regression Trees/Bagging/Random Forests
```{r} 
# fit regression tree
initial_tree = tree(price ~ ., data = training)
summary(initial_tree)
plot(initial_tree)
text(initial_tree, cex = 0.65, digits = 4, pretty = 0)
initial_tree
```


```{r warning =FALSE}
initial_predict = predict(initial_tree, newdata = testing)
tree_MSE = round(mean((testing$price - initial_predict)^2), 4)
print(paste("Test MSE of Initial Tree: ", tree_MSE))

# cross validation
initial_cv = cv.tree(initial_tree)
ggplot() +
  geom_point(aes(x = initial_cv$size, y = initial_cv$dev)) + 
  geom_line(aes(x = initial_cv$size, y = initial_cv$dev)) + 
  ylab("CV Error Rate") + xlab("Size") + ggtitle("CV Classification Error Rate")

# pruned by CV
prune_initial <- prune.tree(initial_tree, best = 8)
plot(prune_initial)
text(prune_initial, cex = 0.65, digits = 4, pretty = 0)
```

Bagging: 
mtry = uses all the variables/predictors 
```{r}
bag_fit <- randomForest(price ~ ., data = training, mtry = ncol(training) - 1, importance = TRUE)
```

```{r fig.height=5, fig.width=9}
bag_predict = predict(bag_fit, testing, type = "response")
bag_MSE = round(mean((testing$price - bag_predict)^2), 4)
print(paste("Test MSE of Bagging: ", bag_MSE))

ggplot() + 
  geom_point(aes(x = bag_predict, y = testing$price), alpha = 0.25) + 
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("Performance of Bagging on Test Set")

data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```


Random Forests:
```{r}
rf_fit <- randomForest(price ~ ., data = training, mtry = sqrt(ncol(training) - 1), importance = TRUE)
```

```{r}
rf_predict = predict(rf_fit, testing, type = "response")
rf_MSE = round(mean((testing$price - rf_predict)^2), 4)
print(paste("Test MSE of Random Forest: ", rf_MSE))

ggplot() + 
  geom_point(aes(x = rf_predict, y = testing$price), alpha = 0.25) + 
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("Performance of Bagging on Test Set")

data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("% Decreasing MSE of Random Forest") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Node Purity of Random Forest") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  


```


Boosting:
```{r}
lambdas = seq(0, 0.5, length.out = 100)
trainErrors = rep(NA, length(lambdas))
testErrors = rep(NA, length(lambdas))
for(i in 1:length(lambdas)){
  boost = gbm(price ~ ., data = training, n.trees = 1000, shrinkage = lambdas[i], distribution = "gaussian")
  trainPred = predict(boost, training, n.trees = 1000)
  testPred = predict(boost, testing, n.trees = 1000)
  trainErrors[i] = mean((training$price - trainPred)^2)
  testErrors[i] = mean((testing$price - testPred)^2)
}
```


```{r}
data.frame(x = lambdas, y = trainErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Training MSE") + ggtitle("Shrinkage vs Training MSE")

data.frame(x = lambdas, y = testErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Test MSE") + ggtitle("Shrinkage vs Test MSE")

```

```{r}
boosted = gbm(price ~ ., data = training, n.trees = 5000, shrinkage = lambdas[which.min(testErrors)], distribution = "gaussian")
summary(boosted)

boosted_pred = predict(boosted, testing)
boosted_MSE = round(mean((testing$price - boosted_pred)^2), 4)
print(paste("Testing MSE for Boosted Model:", boosted_MSE))
```

## MSE Table
```{r}
method = c("Linear Regression", "PCR", "PLS", "Splines", "GAM", "Trees", "Bagging", "Random Forest", "Boosting")
MSEnumbers = c(linearReg_kfold_MSE, PCR_mse, PLS_mse, df_reviews[3,2], gam_MSE, tree_MSE, bag_MSE, rf_MSE, boosted_MSE)

finalMSE = data.frame(Methods = method, MSE = round(MSEnumbers,4), MSE_Dollars = round(exp(MSEnumbers), 2))
finalMSE
```


## Final Model
Load in testing_data.csv
```{r}
final_testing_data = read.csv("testing_data.csv")
testing_df = final_testing_data %>% 
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X)) %>%
  mutate(first_review = as.Date(final_testing_data$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(final_testing_data$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(final_testing_data$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", final_testing_data$host_response_rate))/100)

testing_df = testing_df %>%
  mutate(first_review_year = as.Date(cut(testing_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(testing_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(testing_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

testing_df = testing_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

testing_df = testing_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee)) %>%
  select(-c(first_review, last_review, host_since)) %>%
  mutate(price = 0) 

testing_df = testing_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  filter(property_type != "Earth House")

testing_df = testing_df[complete.cases(testing_df), ]


nrow(testing_df)
head(testing_df)
```


```{r}
str(training_df)
str(testing_df)

training_df %>%
  group_by(property_type) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

testing_df %>%
  group_by(property_type) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

```

```{r}
levels(testing_df$property_type) <- levels(training_df$property_type)
final_fit <- randomForest(price ~ ., data = training_df, mtry = sqrt(ncol(training_df) - 1), importance = TRUE, ntree = 500)
final_predict = predict(final_fit, testing_df, type = "response")
```

```{r}
head(final_predict)
length(final_predict)
nrow(testing_df)
testing_df$price = final_predict
```




## Results
Use best model and predict prices for a certain city and plot based on price (heatmap?)










