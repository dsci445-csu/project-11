---
title: "Predicting AirBnB Rental Rates"
author: "Trevor Isaacson, Jonathan Olavarria, Jasmine DeMeyer"
date: "12/10/2021"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE, echo = FALSE}
set.seed(445)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5, fig.height=3) 
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(e1071)
library(rmarkdown)
library(glmnet)
library(knitr)
library(leaps)
library(tree)
library(dplyr)
library(caret)
library(gbm)
library(randomForest)
library(GGally)
library(pls)
```

```{r}
original_df = read.csv("training_data.csv")
nrow(original_df)
head(original_df)
```

probably don't want the individual id of each place, its website url, description, amenities, name, zipcode (we have lat/long and city) and converted log_price to price
```{r}
training_df = original_df %>% 
  mutate(price = log_price)  %>%
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X, log_price)) %>%
  mutate(first_review = as.Date(original_df$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(original_df$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(original_df$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", original_df$host_response_rate))/100)
```

```{r}
head(training_df)
```

```{r}
# update 3 date columns to track the start of the week instead of the individual date
# create 3 new columns to track dates in the form of month instead of individual date
training_df = training_df %>%
  mutate(first_review_year = as.Date(cut(training_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(training_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(training_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

training_df = training_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

training_df = training_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee))

training_df = training_df[complete.cases(training_df), ]
```

```{r}
# these appear in training set and not testing set so I took them out
training_df <- training_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Island") %>%
  filter(property_type != "Tent") %>%
  filter(property_type != "Treehouse") %>%
  filter(property_type != "Yurt") %>%
  filter(property_type != "Hut") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  select(-c(first_review, last_review, host_since))

head(training_df)
```

# Variable Descriptions:
## Price
```{r}
ggplot(data = training_df) +
  geom_histogram(aes(price), bins = 25) +
  ggtitle("Listing Price ($)") + 
  geom_vline(xintercept = median(training_df$price), color = "blue", lty = 2)

```

## Property Type
```{r}
training_df %>%
  group_by(property_type) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(room_type) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()
```

## City
```{r}
training_df %>%
  group_by(city) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total))

# latitude and longitude
ggplot(data = training_df) +
  geom_point(aes(x = longitude, y = latitude, color = city)) +
  ggtitle("Latitude and Longitude")

```



## Property Details
```{r}
training_df %>%
  group_by(accommodates) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(bedrooms) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(bathrooms) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(bed_type) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

```

## Host details
```{r}
training_df %>%
  group_by(host_has_profile_pic) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(host_identity_verified) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(instant_bookable) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()

training_df %>%
  group_by(cancellation_policy) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) 

training_df %>%
  group_by(host_response_rate) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  kable()
```


## Variable Selection?
Should we first choose which variables should be included in everybody's models or have everybody make their two models and then variable select based on the best model they found? 

Correlation Plot?
```{r}
head(training_df)
# Original df
ggcorr(original_df)
```






## Models
Do we want to split the training_data in half and make one half training and the other half available to use for CV testing and then the testing_data.csv is used for the final model predictions since it doesn't have a price column?
```{r}
set.seed(11)
trn = sample(seq_len(nrow(training_df)), 9000)
training = training_df[trn, ]
testing = training_df[-trn, ]
str(testing)
```

Baseline: Linear Regression
```{r}
linear = lm(price ~ ., data = training)
summary(linear)
```

```{r}
linear_predict = predict(linear, newdata = testing, type = "response")
MSE_testing = (testing$price - linear_predict)^2
print(paste("MSE of Testing Set: ", mean(MSE_testing)))
```

Leave One Out Cross Validation on Linear Regression Model:
```{r warning = FALSE}
n = 107
results_LOOCV = c()
for(i in seq_len(n)){
  trn <- seq_len(n) != i
  
  #fit model
  l = lm(price ~ ., data = training_df[trn, ])
  
  # predict on validation set
  pred = predict(l, training_df[!trn, ])
  
  # estimate test MSE
  true_price = training_df[!trn, ]$price
  
  results_LOOCV[i] = (true_price - pred)^2
}

print(paste("Leave One Out Cross Validation: ", round(mean(results_LOOCV), 2)))
```

k-Fold Cross Validation on Linear Regression Model:
```{r warning = FALSE}
k = 10
folds = sample(seq_len(k), nrow(training_df) , replace = TRUE)
results_kfold = c()
for(i in seq_len(k)){
  trn <- folds != i
  
  #fit model
  l = lm(price ~ ., data = training_df[trn, ])
  
  # predict on validation set
  pred = predict(l, training_df[!trn, ])
  
  # estimate test MSE
  true_price = training_df[!trn, ]$price
  
  results_kfold[i] = (true_price - pred)^2
}

print(paste("k-fold Cross Validation: ", round(mean(results_kfold), 2)))
```


JJ: PCR and PLS

```{r}
#Fit PCR
m0 <- pcr(price~., data = training_df,ncomp= 30, validation = "CV")
mse <- MSEP(m0)
data.frame(M = mse$comps, mse = t(as.data.frame(mse$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse))
mse$val
summary(m0)
```


Jasmine: Regression Splines/Generalized Additive Models
```{r}
library(splines)
library(boot)
head(training_df)
ggpairs(training_df, columns = c(2:8, 19))
ggpairs(training_df, columns = c(9:14, 19))
ggpairs(training_df, columns = 15:22)

#spline on accommodates
fit <- glm(price ~ bs(accommodates, df = 3), data = training_df)
cvs <- cv.glm(training_df, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(accommodates, df = 4), data = training_df)
cvs2 <- cv.glm(training_df, fit, K = 10)$delta[1]

fit3 <- glm(price ~ bs(accommodates, df = 5), data = training_df)
cvs3 <- cv.glm(training_df, fit, K = 10)$delta[1]
  
fit4 <- glm(price ~ bs(accommodates, df = 6), data = training_df)
cvs4 <- cv.glm(training_df, fit, K = 10)$delta[1]
  
fit5 <- glm(price ~ bs(accommodates, df = 7), data = training_df)
cvs5 <- cv.glm(training_df, fit, K = 10)$delta[1]

fit6 <- glm(price ~ bs(accommodates, df = 8), data = training_df)
cvs6 <- cv.glm(training_df, fit, K = 10)$delta[1]

fit7 <- glm(price ~ bs(accommodates, df = 9), data = training_df)
cvs7 <- cv.glm(training_df, fit, K = 10)$delta[1]

fit8 <- glm(price ~ bs(accommodates, df = 10), data = training_df)
cvs8 <- cv.glm(training_df, fit, K = 10)$delta[1]

degfree <- c(3, 4, 5, 6, 7, 8, 9, 10)
cv <- c(cvs, cvs2, cvs3, cvs4, cvs5, cvs6, cvs7, cvs8)
df <- data.frame(degfree, cv)

#spline on review_scores_rating
fit <- glm(price ~ bs(review_scores_rating, df = 3), data = training_df)
cvs <- cv.glm(training_df, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(review_scores_rating, df = 4), data = training_df)
cvs2 <- cv.glm(training_df, fit, K = 10)$delta[1]

fit3 <- glm(price ~ bs(review_scores_rating, df = 5), data = training_df)
cvs3 <- cv.glm(training_df, fit, K = 10)$delta[1]
  
fit4 <- glm(price ~ bs(review_scores_rating, df = 6), data = training_df)
cvs4 <- cv.glm(training_df, fit, K = 10)$delta[1]
  
fit5 <- glm(price ~ bs(review_scores_rating, df = 7), data = training_df)
cvs5 <- cv.glm(training_df, fit, K = 10)$delta[1]

fit6 <- glm(price ~ bs(review_scores_rating, df = 8), data = training_df)
cvs6 <- cv.glm(training_df, fit, K = 10)$delta[1]

fit7 <- glm(price ~ bs(review_scores_rating, df = 9), data = training_df)
cvs7 <- cv.glm(training_df, fit, K = 10)$delta[1]

fit8 <- glm(price ~ bs(review_scores_rating, df = 10), data = training_df)
cvs8 <- cv.glm(training_df, fit, K = 10)$delta[1]

degfree <- c(3, 4, 5, 6, 7, 8, 9, 10)
cv <- c(cvs, cvs2, cvs3, cvs4, cvs5, cvs6, cvs7, cvs8)
df <- data.frame(degfree, cv)
```
Choose spline on accommodates with df = 5

```{r}
#fit GAM
```





Trevor: Regression Trees/Bagging/Random Forests
```{r} 
# fit regression tree
initial_tree = tree(price ~ ., data = training)
summary(initial_tree)
plot(initial_tree)
text(initial_tree, cex = 0.65, digits = 4, pretty = 0)
initial_tree
```


```{r warning =FALSE}
initial_predict = predict(initial_tree, newdata = testing)
print(paste("Test MSE of Initial Tree: ", round(mean((testing$price - initial_predict)^2), 4)))

# cross validation
initial_cv = cv.tree(initial_tree)
ggplot() +
  geom_point(aes(x = initial_cv$size, y = initial_cv$dev)) + 
  geom_line(aes(x = initial_cv$size, y = initial_cv$dev)) + 
  ylab("CV Error Rate") + xlab("Size") + ggtitle("CV Classification Error Rate")

# pruned by CV
prune_initial <- prune.tree(initial_tree, best = 8)
plot(prune_initial)
text(prune_initial, cex = 0.65, digits = 4, pretty = 0)
```

Bagging: 
mtry = uses all the variables/predictors 
```{r}
bag_fit <- randomForest(price ~ ., data = training, mtry = ncol(training) - 1, importance = TRUE)
```

```{r fig.height=5, fig.width=9}
bag_predict = predict(bag_fit, testing, type = "response")
print(paste("Test MSE of Bagging: ", round(mean((testing$price - bag_predict)^2), 4)))

ggplot() + 
  geom_point(aes(x = bag_predict, y = testing$price), alpha = 0.25) + 
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("Performance of Bagging on Test Set")

data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("% Decreasing MSE of Bagging") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Node Purity of Bagging") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```

Random Forests:
```{r}
rf_fit <- randomForest(price ~ ., data = training, mtry = sqrt(ncol(training) - 1), importance = TRUE)
```

```{r}
rf_predict = predict(rf_fit, testing, type = "response")
print(paste("Test MSE of Random Forest: ", round(mean((testing$price - rf_predict)^2), 4)))

ggplot() + 
  geom_point(aes(x = rf_predict, y = testing$price), alpha = 0.25) + 
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("Performance of Bagging on Test Set")

data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("% Decreasing MSE of Random Forest") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Node Purity of Random Forest") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  


```


Boosting:
```{r}
lambdas = seq(0, 0.5, length.out = 100)
trainErrors = rep(NA, length(lambdas))
testErrors = rep(NA, length(lambdas))
for(i in 1:length(lambdas)){
  boost = gbm(price ~ ., data = training, n.trees = 1000, shrinkage = lambdas[i], distribution = "gaussian")
  trainPred = predict(boost, training, n.trees = 1000)
  testPred = predict(boost, testing, n.trees = 1000)
  trainErrors[i] = mean((training$price - trainPred)^2)
  testErrors[i] = mean((testing$price - testPred)^2)
}
```


```{r}
data.frame(x = lambdas, y = trainErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Training MSE") + ggtitle("Shrinkage vs Training MSE")

data.frame(x = lambdas, y = testErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Test MSE") + ggtitle("Shrinkage vs Test MSE")

```

```{r}
boosted = gbm(price ~ ., data = training, n.trees = 5000, shrinkage = lambdas[which.min(testErrors)], distribution = "gaussian")
summary(boosted)

boosted_pred = predict(boosted, testing)
print(paste("Testing MSE for Boosted Model:", round(mean((testing$price - boosted_pred)^2), 4)))
```



## Final Model







## Results
Use best model and predict prices for a certain city and plot based on price (heatmap?)










