---
title: "Predicting AirBnB Rental Rates"
author: "Trevor Isaacson, Jonathan Olavarria, Jasmine DeMeyer"
date: "12/10/2021"
output: pdf_document
---

```{r setup, include=FALSE, echo = FALSE}
set.seed(445)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5, fig.height=3) 
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(e1071)
library(rmarkdown)
library(glmnet)
library(knitr)
library(leaps)
library(tree)
library(dplyr)
library(caret)
library(gbm)
library(randomForest)
library(GGally)
library(pls)
library(splines)
library(boot)
```

```{r}
original_df = read.csv("training_data.csv")
```

```{r}
training_df = original_df %>% 
  mutate(price = log_price)  %>%
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X, log_price)) %>%
  mutate(first_review = as.Date(original_df$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(original_df$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(original_df$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", original_df$host_response_rate))/100)

training_df = training_df %>%
  mutate(first_review_year = as.Date(cut(training_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(training_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(training_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

training_df = training_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

training_df = training_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee))

training_df = training_df[complete.cases(training_df), ]

training_df <- training_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Island") %>%
  filter(property_type != "Tent") %>%
  filter(property_type != "Treehouse") %>%
  filter(property_type != "Yurt") %>%
  filter(property_type != "Hut") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  select(-c(first_review, last_review, host_since))
```

# Introduction 





# The Data     
|   As a company, AirBnB is very open and transparent with the data they collect about their rental properties.  They provide data about rental spaces in their system for cities and countries all over the world.  Because of this, we were able to find a large dataset on Kaggle with AirBnB listings in major US cities including New York City, Los Angeles, San Francisco and others.  The dataset available on Kaggle has over 74,000 entries and was used as a competition a few years ago.  For the sake of time and processing, we trimmed our training data to about 17,500 entries and our test data to about 5,000 entries.  We did this by taking a random sample of the provided training data.  This allowed for easier access and faster processing while maintaining a large amount of data and individual AirBnB listings.  

|   The original dataset contained 30 variables about each listing.  Due to high correlations and lack of relevancy, our final dataset consisted of twenty-two variables.  Those twenty-two variables can be split into four categories: property, location, host and host reviews.  

Property includes:

  - price: listing price    
      - Because the original price data is very heavily skewed, we needed to log transform the prices.  As shown in the histogram, we have a very heavy right tail because there are a  some listings with very high prices compared to the median price of $110 (blue line).  This non-normal shape and distribution is clearly evident in the Q-Q plot.  The observations clearly curve away from the line depicting how skewed the distribution is.  

```{r fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
ggplot(data = training_df) +
  geom_histogram(aes(exp(price)), bins = 50) +
  ggtitle("Listing Price ($)") + 
  geom_vline(xintercept = median(exp(training_df$price)), color = "blue", lty = 2) + 
  xlab("price")

ggplot(data = training_df, aes(sample = exp(price))) +
  stat_qq() + stat_qq_line() + ggtitle("Q-Q Plot of Original Price") 

```

```{r fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
ggplot(data = training_df) +
  geom_histogram(aes(price), bins = 25) +
  ggtitle("Log of Listing Price ($)") + 
  geom_vline(xintercept = median(training_df$price), color = "blue", lty = 2) + xlab("log(price)")

ggplot(data = training_df, aes(sample = price)) +
  stat_qq() + stat_qq_line() + ggtitle("Q-Q Plot of Log Price") 
```   

|   By applying a log transformation, we now have a more normal shaped distribution.  Our Q-Q plot shows some evidence of a right tail but this can be expected since the original distribution is very right tailed.  

  - property_type: defines the type of property listed
      - There are 21 different types ranging from apartments, houses, and condos to boats, cabins, hostels and even castles
  
  - room_type: defines type of rental within the property
      - Includes entire home/apt, private room and shared room
  
  - accommodates: number of people the property can comfortably accommodate
  
  - bedrooms: number of bedrooms within the property
  
  - beds: number of beds within the property
  
  - bed_type: type of bed available
      - This includes a Real Bed, Futon, Pull-out Sofa,	Airbed or Couch
      - Only 463 listings have something other than a Real Bed
  
  - bathrooms: number of bathrooms within the property
  
```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
training_df %>%
  group_by(accommodates) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  ggplot() + 
  geom_point(aes(x = accommodates, y = Total)) + 
  geom_line(aes(x = accommodates, y = Total)) +
  ggtitle("Accommodates Distribution") + scale_x_continuous(breaks = c(0:16))

training_df %>%
  group_by(beds) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  ggplot() + 
  geom_point(aes(x = beds, y = Total)) + 
  geom_line(aes(x = beds, y = Total)) +
  ggtitle("Beds Distribution") + scale_x_continuous(breaks = c(0:20))
```

```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
training_df %>%
  group_by(bedrooms) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  ggplot() +
  geom_point(aes(x = bedrooms, y = Total)) + 
  geom_line(aes(x = bedrooms, y = Total)) +
  ggtitle("Bedrooms Distribution") + scale_x_continuous(breaks = c(0:10))

training_df %>%
  group_by(bathrooms) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  ggplot() +
  geom_point(aes(x = bathrooms, y = Total)) + 
  geom_line(aes(x = bathrooms, y = Total)) +
  ggtitle("Bathrooms Distribution") + scale_x_continuous(breaks = c(0:8))
```
  
Location includes:

  - city: Location of listing
  
  - latitude and longitude: latitude and longitude coordinates of the listing
  
```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
training_df %>%
  group_by(city) %>%
  ggplot() + geom_bar(aes(city)) + 
  ggtitle("City Distribution")

latlong = training_df[training_df$longitude > -73, ]
ggplot(data = latlong) +
  geom_point(aes(x = longitude, y = latitude, color = city)) +
  ggtitle("Listings in Boston")
```
  
Host includes: 

  - cancellation_policy: strictness of cancellation policy set by the host
      - Levels include strict, moderate, flexible, super_strict_30	and super_strict_60
  
  - cleaning_fee: TRUE/FALSE determines if host charges a cleaning fee
  
  - host_has_profile_pic: TRUE/FALSE determines if the host has uploaded a picture to their profile
  
  - host_identify_verified: TRUE/FALSE determines if the host's identity has been verified by AirBnB
  
  - instant_bookable: TRUE/FALSE determines if the property can be booked in short notice
  
  - host_response_rate: how often does the host reply to potential clients?

```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="33%"}
training_df %>%
  group_by(cancellation_policy) %>%
  ggplot() + geom_bar(aes(cancellation_policy)) + 
  ggtitle("Cancellation Policy")

training_df %>%
  group_by(cleaning_fee) %>%
  ggplot() + geom_bar(aes(cleaning_fee)) + 
  ggtitle("Cleaning Fee?")

training_df %>%
  group_by(host_has_profile_pic) %>%
  ggplot() + geom_bar(aes(host_has_profile_pic)) + 
  ggtitle("Profile Pic?")
```


```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="33%"}
training_df %>%
  group_by(host_identity_verified) %>%
  ggplot() + geom_bar(aes(host_identity_verified)) + 
  ggtitle("Identify Verified?")

training_df %>%
  group_by(instant_bookable) %>%
  ggplot() + geom_bar(aes(instant_bookable)) + 
  ggtitle("Instant Bookable?")
```


Host Reviews:

  - number_of_reviews: Number of reviews the host has received
  
  - review_scores_rating: average review rating for the host and property
  
  - first_review_year: year of the first review
  
  - last_review_year: year of most recent review
  
  - host_since_year: year the property was first listed on AirBnB
  
As a group, we felt these twenty-two predictors were all relevant and important in helping predict price.


# Models   
|   There are have a lot of machine learning methods discussed this past semester and we wanted to incorporate some of our favorites into our research.  Thus, we have included linear regression, splines, general additive models, PCR, PLS, trees, bagging, random forests and bagging.   
```{r}
trn = sample(seq_len(nrow(training_df)), 9000)
training = training_df[trn, ]
testing = training_df[-trn, ]
```
|   In order to both train and test using the training data file, we needed to split the 17,500 total observations into roughly a 50/50 split.  To do this, we took a random sample of 9000 observations and made that the training set.  From now on, this random sample with be referred to as the training set. The remaining 7500 observations became our testing set for determining the the performance of each method.  

# Regression      
|   To begin, we started with a simple multiple linear regression model.  We wanted to give ourselves a baseline mean squared error value and because linear regression is the easiest to apply and interpret, we determined this was the best place to start.  The model was fit using all twenty-two variables and the training set.  

```{r out.width="33%", echo=FALSE}
linear = lm(price ~ ., data = training)
linear_predict = predict(linear, newdata = testing, type = "response")
MSE_testing = (testing$price - linear_predict)^2
summaryLinear = summary(linear)
print(paste("R^2:", round(summaryLinear$r.squared, 3), ", Adjusted R^2: ", round(summaryLinear$adj.r.squared, 3)))
print(paste("MSE of Testing Set: ", round(mean(MSE_testing), 4)))
```

```{r warning = FALSE}
n = 107
results_LOOCV = c()
for(i in seq_len(n)){
  trn <- seq_len(n) != i
  
  #fit model
  l = lm(price ~ ., data = training_df[trn, ])
  
  # predict on validation set
  pred = predict(l, training_df[!trn, ])
  
  # estimate test MSE
  true_price = training_df[!trn, ]$price
  
  results_LOOCV[i] = (true_price - pred)^2
}
print(paste("Leave One Out Cross Validation: ", round(mean(results_LOOCV), 4)))

k = 10
folds = sample(seq_len(k), nrow(training_df) , replace = TRUE)
results_kfold = c()
for(i in seq_len(k)){
  trn <- folds != i
  
  #fit model
  l = lm(price ~ ., data = training_df[trn, ])
  
  # predict on validation set
  pred = predict(l, training_df[!trn, ])
  
  # estimate test MSE
  true_price = training_df[!trn, ]$price
  
  results_kfold[i] = (true_price - pred)^2
}

linearReg_kfold_MSE = round(mean(results_kfold), 4)
print(paste("k-fold Cross Validation: ", linearReg_kfold_MSE))
```

|   As shown in the results above, our linear regression model was able to set a good baseline for future methods with a mean squared error of 0.1652.  With our linear regression model, we also applied some cross validation.  For Leave One Out Cross Validation, we achieved a mean squared error 0.1829 and applying k-fold cross validation with k = 10, we achieved a mean square error of 0.3063.  Clearly, Leave One Out Cross Validation performed better than the k-fold cross validation.  


# Regression Splines/Generalized Additive Models          


```{r warning = FALSE}
fit <- glm(price ~ bs(accommodates, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(accommodates, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(accommodates, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_accomodates <- data.frame(degfree, cv)

#spline on review_scores_rating
fit <- glm(price ~ bs(review_scores_rating, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(review_scores_rating, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(review_scores_rating, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_reviews <- data.frame(degfree, cv)

#spline on bathrooms
fit <- glm(price ~ bs(bathrooms, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(bathrooms, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(bathrooms, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_bathrooms <- data.frame(degfree, cv)

#spline on bedrooms
fit <- glm(price ~ bs(bedrooms, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(bedrooms, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(bedrooms, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_bedrooms <- data.frame(degfree, cv)


df = c(3, 4, 5)
splines = data.frame(df = df, accommodates = df_accomodates$cv, reviews = df_reviews$cv, bedrooms = df_bedrooms$cv, bathrooms = df_bathrooms$cv)
splines = round(splines, 4)
splines
```


```{r warning = FALSE}
#fit GAM
gammod <- lm(price ~ . + bs(accommodates, df = 2) + bs(review_scores_rating, df = 4) + bs(bathrooms, df = 4) + bs(bedrooms, df = 3), data = training)
summary(gammod)

gam_predict = predict(gammod, newdata = testing)
gam_MSE = round(mean((testing$price - gam_predict)^2), 4)
print(paste("Test MSE of GAM: ", gam_MSE))
```




# PCR and PLS      

```{r}
#Fit PCR
m0 <- pcr(price~., data = training,ncomp= 20, validation = "CV")
mse <- MSEP(m0)
data.frame(M = mse$comps, mse = t(as.data.frame(mse$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse))
# based off of plot we will select 15 components (elbow method) = 99.97% of variance explained
mse$val
summary(m0)

# R^2 vs Principal Components Plot
validationplot(m0, val.type = "R2")

# Test Set Predictions
pcr_pred <- predict(m0, testing, ncomp = 15)

# Prediction Plot
predplot(m0, ncomp = 15)

# Check for NA values
sum(is.na(pcr_pred))
sum(is.na(testing$price))

# Calculate Test Set MSE
PCR_mse <- round(mean((testing$price - pcr_pred)^2), 4)
PCR_mse

```


```{r}
# Fit PLS
m1 <- plsr(price~., data=training, validation="CV",ncomp=20)

# Plot MSE
mse1 <- MSEP(m1)
data.frame(M = mse1$comps, mse = t(as.data.frame(mse1$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse))

# Number of Components chosen is 10 = 99.93% of Variance Explained
summary(m1)

# Test Set Predictions
pls_pred <- predict(m1, testing, ncomp = 10)

# Prediction Plot
predplot(m1, ncomp = 10)

# Check for NA values
sum(is.na(pls_pred))

# Calculate Test Set MSE
PLS_mse <- round(mean((testing$price - pls_pred)^2), 4)
PLS_mse

```


# Trees
|   The next method we used was decision trees.  Fitting the fit using all predictors, we obtained a tree where five variables were actually used in the tree construction.  Those variables were room_type, longitude, bathrooms, city and bedrooms.  The tree has eight terminal nodes and obtained a test mean squared error of 0.1926.  Because the relationship between the features and the response is approximately linear, linear regression obtains a better MSE than the regression tree.  The cv.tree function was used to perform cross-validation to determine the optimal level of tree complexity.  The optimal level was 8 and then using the prune.tree function, we attempted to prune our tree to the chosen complexity but found that our original tree was the same as the pruned tree.  The table below shows the log prices converted to dollars to help explain the terminal node values.  Node 1 is the farthest node on the left.  

```{r message=FALSE, fig.width=5, fig.height=3,  fig.show='hold'} 
# fit regression tree
initial_tree = tree(price ~ ., data = training)
initial_predict = predict(initial_tree, newdata = testing)
tree_MSE = round(mean((testing$price - initial_predict)^2), 4)
print(paste("Test MSE of Tree: ", tree_MSE))
```
```{r message=FALSE, fig.width=7, fig.height=4, fig.show='hold'} 
plot(initial_tree)
text(initial_tree, cex = 0.55, digits = 4, pretty = 0)
```

```{r message=FALSE, fig.width=5, fig.height=3,  fig.show='hold'} 
node = c(1,2,3,4,5,6,7,8)
log_price = c(4.65, 4.259, 3.753, 4.807, 5.197, 4.905, 5.379, 5.843)
price = round(exp(log_price),2)
tree_explained = data.frame(TerminalNode = node, Log_Price = log_price, Price = price)
kable(tree_explained)
```



# Bagging
|   Because decision trees suffer from high variance, we then moved onto bagging or bootstrap aggregation to see if we could lower the MSE of our trees.  Although bagging decision trees can be slow as it has to average hundreds or thousands of trees together, it won't lead to any overfitting.  We performed bagging on the training set to predict price and found that room type, bathrooms and latitude/longitude where the most important variables.  Our bagging model was able to obtain a MSE of 0.1293 and was found to be one of our best methods. 

```{r}
bag_fit <- randomForest(price ~ ., data = training, mtry = ncol(training) - 1, importance = TRUE)
bag_predict = predict(bag_fit, testing, type = "response")
bag_MSE = round(mean((testing$price - bag_predict)^2), 4)
print(paste("Test MSE of Bagging: ", bag_MSE))
```

```{r echo = FALSE, message=FALSE, fig.show='hold', out.width="50%"}
data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```

|   The plot below shows an Actual vs Predicted plot for the predicted values using bagging.  In general, the data points move positively along the diagonal line with a slight horizontal tilt in the data points compared to the horizontal line.  There are a few outliers especially on the right side of the plot.  It appears bagging is predicting slightly higher than the actual values for prices less than 5 and slightly lower than the actual values for prices more than 5.  However, this plot shows the overall effectiveness for bagging.  


```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3, fig.show='hold'}
ggplot() + 
  geom_point(aes(x = testing$price, y = bag_predict), alpha = 0.25) + 
  geom_abline(slope = 1, intercept = 0) + xlab("Predicted") + ylab("Predicted") +
  ggtitle("Performance of Bagging on Test Set")
```



# Random Forests
|   Next, we wanted to determine if random forests could provide an improvement over the bagged trees and see if decorrelating the trees will lead to better MSE result.  The choice of m or the size of the predictor subset was set to the square root of the total predictor set.  

```{r}
rf_fit <- randomForest(price ~ ., data = training, mtry = sqrt(ncol(training) - 1), importance = TRUE)
rf_predict = predict(rf_fit, testing, type = "response")
rf_MSE = round(mean((testing$price - rf_predict)^2), 4)
print(paste("Test MSE of Random Forest: ", rf_MSE))
```

```{r echo = FALSE, message=FALSE, fig.show='hold', out.width="50%"}
data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```




```{r}
ggplot() + 
  geom_point(aes(x = rf_predict, y = testing$price), alpha = 0.25) + 
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("Performance of Bagging on Test Set")
```



# Boosting

```{r}
lambdas = seq(0, 0.5, length.out = 100)
trainErrors = rep(NA, length(lambdas))
testErrors = rep(NA, length(lambdas))
for(i in 1:length(lambdas)){
  boost = gbm(price ~ ., data = training, n.trees = 100, shrinkage = lambdas[i], distribution = "gaussian")
  trainPred = predict(boost, training, n.trees = 100)
  testPred = predict(boost, testing, n.trees = 100)
  trainErrors[i] = mean((training$price - trainPred)^2)
  testErrors[i] = mean((testing$price - testPred)^2)
}
```


```{r echo = FALSE, message=FALSE, fig.show='hold', out.width="50%"}
data.frame(x = lambdas, y = trainErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Training MSE") + ggtitle("Shrinkage vs Training MSE")

data.frame(x = lambdas, y = testErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Test MSE") + ggtitle("Shrinkage vs Test MSE")

```

```{r fig.height=5}
boosted = gbm(price ~ ., data = training, n.trees = 5000, shrinkage = lambdas[which.min(testErrors)], distribution = "gaussian")
summary(boosted)

boosted_pred = predict(boosted, testing)
boosted_MSE = round(mean((testing$price - boosted_pred)^2), 4)
print(paste("Testing MSE for Boosted Model:", boosted_MSE))
```



# MSE Results             
```{r}
method = c("Linear Regression", "PCR", "PLS", "Splines", "GAM", "Trees", "Bagging", "Random Forest", "Boosting")
MSEnumbers = c(linearReg_kfold_MSE, PCR_mse, PLS_mse, df_reviews[3,2], gam_MSE, tree_MSE, bag_MSE, rf_MSE, boosted_MSE)

finalMSE = data.frame(Methods = method, MSE = round(MSEnumbers,4), MSE_Dollars = round(exp(MSEnumbers), 2))
kable(finalMSE)
```



# City Prediction     
|    Our original data set is from a past and completed Kaggle competition.  Thus, the data was split into training and testing files.  The testing file has no response column included because that is how they would determine a winner.  Even though our testing results can't be compared to the actual prices, we can still use it for testing our final model.  

```{r}
final_testing_data = read.csv("testing_data.csv")
testing_df = final_testing_data %>% 
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X)) %>%
  mutate(first_review = as.Date(final_testing_data$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(final_testing_data$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(final_testing_data$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", final_testing_data$host_response_rate))/100)

testing_df = testing_df %>%
  mutate(first_review_year = as.Date(cut(testing_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(testing_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(testing_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

testing_df = testing_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

testing_df = testing_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee)) %>%
  select(-c(first_review, last_review, host_since)) %>%
  mutate(price = 0) 

testing_df = testing_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  filter(property_type != "Earth House")

testing_df = testing_df[complete.cases(testing_df), ]
```

```{r}
# levels(testing_df$property_type) <- levels(training_df$property_type)
# final_fit <- randomForest(price ~ ., data = training_df, mtry = sqrt(ncol(training_df) - 1), importance = TRUE, ntree = 500)
# final_predict = predict(final_fit, testing_df, type = "response")
# final_predict
```






\newpage

# References:













