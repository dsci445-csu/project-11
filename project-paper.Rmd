---
title: "Predicting AirBnB Rental Rates"
author: "Trevor Isaacson, Jonathan Olavarria, Jasmine DeMeyer"
date: "12/10/2021"
output: pdf_document
---

```{r setup, include=FALSE, echo = FALSE}
set.seed(445)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5, fig.height=3) 
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(e1071)
library(rmarkdown)
library(glmnet)
library(knitr)
library(leaps)
library(tree)
library(dplyr)
library(caret)
library(gbm)
library(randomForest)
library(GGally)
library(pls)
library(splines)
library(boot)
```

```{r}
original_df = read.csv("training_data.csv")
```

```{r}
training_df = original_df %>% 
  mutate(price = log_price)  %>%
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X, log_price)) %>%
  mutate(first_review = as.Date(original_df$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(original_df$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(original_df$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", original_df$host_response_rate))/100)

training_df = training_df %>%
  mutate(first_review_year = as.Date(cut(training_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(training_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(training_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

training_df = training_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

training_df = training_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee))

training_df = training_df[complete.cases(training_df), ]

training_df <- training_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Island") %>%
  filter(property_type != "Tent") %>%
  filter(property_type != "Treehouse") %>%
  filter(property_type != "Yurt") %>%
  filter(property_type != "Hut") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  select(-c(first_review, last_review, host_since))
```

# Introduction 





# The Data     
|   As a company, AirBnB is very open and transparent with the data they collect about their rental properties.  They provide data about rental spaces in their system for cities and countries all over the world.  Because of this, we were able to find a large dataset on Kaggle with AirBnB listings in major US cities including New York City, Los Angeles, San Francisco and others.  The dataset available on Kaggle has over 74,000 entries and was used as a competition a few years ago.  For the sake of time and processing, we trimmed our training data to about 17,500 entries and our test data to about 5,000 entries.  We did this by taking a random sample of the provided training data.  This allowed for easier access and faster processing while maintaining a large amount of data and individual AirBnB listings.  

|   The original dataset contained 30 variables about each listing.  Due to high correlations and lack of relevancy, our final dataset consisted of twenty-two variables.  Those twenty-two variables can be split into four categories: property, location, host and host reviews.  

Property includes:

  - price: listing price    
      - Because the original price data is very heavily skewed, we needed to log transform the prices.  As shown in the histogram, we have a very heavy right tail because there are a  some listings with very high prices compared to the median price of $110 (blue line).  This non-normal shape and distribution is clearly evident in the Q-Q plot.  The observations clearly curve away from the line depicting how skewed the distribution is.  

```{r fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
ggplot(data = training_df) +
  geom_histogram(aes(exp(price)), bins = 50) +
  ggtitle("Listing Price ($)") + 
  geom_vline(xintercept = median(exp(training_df$price)), color = "blue", lty = 2) + 
  xlab("price")

ggplot(data = training_df, aes(sample = exp(price))) +
  stat_qq() + stat_qq_line() + ggtitle("Q-Q Plot of Original Price") 

```

```{r fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
ggplot(data = training_df) +
  geom_histogram(aes(price), bins = 25) +
  ggtitle("Log of Listing Price ($)") + 
  geom_vline(xintercept = median(training_df$price), color = "blue", lty = 2) + xlab("log(price)")

ggplot(data = training_df, aes(sample = price)) +
  stat_qq() + stat_qq_line() + ggtitle("Q-Q Plot of Log Price") 
```   

|   By applying a log transformation, we now have a more normal shaped distribution.  Our Q-Q plot shows some evidence of a right tail but this can be expected since the original distribution is very right tailed.  

  - property_type: defines the type of property listed
      - There are 21 different types ranging from apartments, houses, and condos to boats, cabins, hostels and even castles
  
  - room_type: defines type of rental within the property
      - Includes entire home/apt, private room and shared room
  
  - accommodates: number of people the property can comfortably accommodate
  
  - bedrooms: number of bedrooms within the property
  
  - beds: number of beds within the property
  
  - bed_type: type of bed available
      - This includes a Real Bed, Futon, Pull-out Sofa,	Airbed or Couch
      - Only 463 listings have something other than a Real Bed
  
  - bathrooms: number of bathrooms within the property
  
```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
training_df %>%
  group_by(accommodates) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  ggplot() + 
  geom_point(aes(x = accommodates, y = Total)) + 
  geom_line(aes(x = accommodates, y = Total)) +
  ggtitle("Accommodates Distribution") + scale_x_continuous(breaks = c(0:16))

training_df %>%
  group_by(beds) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  ggplot() + 
  geom_point(aes(x = beds, y = Total)) + 
  geom_line(aes(x = beds, y = Total)) +
  ggtitle("Beds Distribution") + scale_x_continuous(breaks = c(0:20))
```

```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
training_df %>%
  group_by(bedrooms) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  ggplot() +
  geom_point(aes(x = bedrooms, y = Total)) + 
  geom_line(aes(x = bedrooms, y = Total)) +
  ggtitle("Bedrooms Distribution") + scale_x_continuous(breaks = c(0:10))

training_df %>%
  group_by(bathrooms) %>%
  summarise(Total = n()) %>%
  arrange(desc(Total)) %>%
  ggplot() +
  geom_point(aes(x = bathrooms, y = Total)) + 
  geom_line(aes(x = bathrooms, y = Total)) +
  ggtitle("Bathrooms Distribution") + scale_x_continuous(breaks = c(0:8))
```
  
Location includes:

  - city: Location of listing
  
  - latitude and longitude: latitude and longitude coordinates of the listing
  
```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
training_df %>%
  group_by(city) %>%
  ggplot() + geom_bar(aes(city)) + 
  ggtitle("City Distribution")

latlong = training_df[training_df$longitude > -73, ]
ggplot(data = latlong) +
  geom_point(aes(x = longitude, y = latitude, color = city)) +
  ggtitle("Listings in Boston")
```
  
Host includes: 

  - cancellation_policy: strictness of cancellation policy set by the host
      - Levels include strict, moderate, flexible, super_strict_30	and super_strict_60
  
  - cleaning_fee: TRUE/FALSE determines if host charges a cleaning fee
  
  - host_has_profile_pic: TRUE/FALSE determines if the host has uploaded a picture to their profile
  
  - host_identify_verified: TRUE/FALSE determines if the host's identity has been verified by AirBnB
  
  - instant_bookable: TRUE/FALSE determines if the property can be booked in short notice
  
  - host_response_rate: how often does the host reply to potential clients?

```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="33%"}
training_df %>%
  group_by(cancellation_policy) %>%
  ggplot() + geom_bar(aes(cancellation_policy)) + 
  ggtitle("Cancellation Policy")

training_df %>%
  group_by(cleaning_fee) %>%
  ggplot() + geom_bar(aes(cleaning_fee)) + 
  ggtitle("Cleaning Fee?")

training_df %>%
  group_by(host_has_profile_pic) %>%
  ggplot() + geom_bar(aes(host_has_profile_pic)) + 
  ggtitle("Profile Pic?")
```


```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="33%"}
training_df %>%
  group_by(host_identity_verified) %>%
  ggplot() + geom_bar(aes(host_identity_verified)) + 
  ggtitle("Identify Verified?")

training_df %>%
  group_by(instant_bookable) %>%
  ggplot() + geom_bar(aes(instant_bookable)) + 
  ggtitle("Instant Bookable?")
```


Host Reviews:

  - number_of_reviews: Number of reviews the host has received
  
  - review_scores_rating: average review rating for the host and property
  
  - first_review_year: year of the first review
  
  - last_review_year: year of most recent review
  
  - host_since_year: year the property was first listed on AirBnB
  
As a group, we felt these twenty-two predictors were all relevant and important in helping predict price.


# Models   
|   There have been a lot of machine learning methods discussed this past semester and we wanted to incorporate some of our favorites into our research.  Thus, we have included linear regression, splines, general additive models, PCR, PLS, trees, bagging, random forests and boosting.   
```{r}
trn = sample(seq_len(nrow(training_df)), 9000)
training = training_df[trn, ]
testing = training_df[-trn, ]
```
|   In order to both train and test using the training data file, since the provided testing file has no response variable, we needed to split the 17,500 total observations into roughly a 50/50 split.  To do this, we took a random sample of 9000 observations and made that the training set.  From now on, this random sample with be referred to as the training set. The remaining 7500 observations became our testing set for determining the the performance of each method.  

# Regression      
|   To begin, we started with a simple multiple linear regression model.  We wanted to give ourselves a baseline mean squared error value and because linear regression is the easiest to apply and interpret, we determined this is the best place to start.  The model is fit using all twenty-two variables and the training set.  

```{r out.width="33%", echo=FALSE}
linear = lm(price ~ ., data = training)
linear_predict = predict(linear, newdata = testing, type = "response")
MSE_testing = (testing$price - linear_predict)^2
summaryLinear = summary(linear)
print(paste("R^2:", round(summaryLinear$r.squared, 3), ", Adjusted R^2: ", round(summaryLinear$adj.r.squared, 3)))
print(paste("MSE of Testing Set: ", round(mean(MSE_testing), 4)))
```

```{r warning = FALSE}
n = 107
results_LOOCV = c()
for(i in seq_len(n)){
  trn <- seq_len(n) != i
  
  #fit model
  l = lm(price ~ ., data = training_df[trn, ])
  
  # predict on validation set
  pred = predict(l, training_df[!trn, ])
  
  # estimate test MSE
  true_price = training_df[!trn, ]$price
  
  results_LOOCV[i] = (true_price - pred)^2
}
print(paste("Leave One Out Cross Validation: ", round(mean(results_LOOCV), 4)))

k = 10
folds = sample(seq_len(k), nrow(training_df) , replace = TRUE)
results_kfold = c()
for(i in seq_len(k)){
  trn <- folds != i
  
  #fit model
  l = lm(price ~ ., data = training_df[trn, ])
  
  # predict on validation set
  pred = predict(l, training_df[!trn, ])
  
  # estimate test MSE
  true_price = training_df[!trn, ]$price
  
  results_kfold[i] = (true_price - pred)^2
}

linearReg_kfold_MSE = round(mean(results_kfold), 4)
print(paste("k-fold Cross Validation: ", linearReg_kfold_MSE))
```

|   As shown in the results above, our linear regression model is able to set a good baseline for future methods with a mean squared error of 0.1652.  With our linear regression model, we also applied some cross validation.  For Leave One Out Cross Validation, we achieved a mean squared error 0.1829 and applying k-fold cross validation with k = 10, we achieved a mean square error of 0.3063.  Clearly, Leave One Out Cross Validation performed better than the k-fold cross validation.  


# Regression Splines/Generalized Additive Models          

|    After attempting linear regression, the next idea was to try fitting regression splines. When looking at some of the more explanatory variables, it was decided to perform a spline on the variables accommodates, review_scores_rating, bathrooms, and bedrooms. A 20-Fold Cross Validation was performed using degrees of freedom ranging typically between 3 and 6. The resulting MSE values generated by these Cross Validation sequences were used to choose the optimal degrees of freedom for each of the splines. The degrees of freedom value with the smallest resulting MSE is what was fit in the spline model.
    These splines were all fit with their optimal degrees of freedom so that we could obtain the resulting MSE values. When comparing the Cross-Validation MSE values for each of the four splines that were fit, it was noted that the best performing spline model was the spline on review_scores_rating with degrees of freedom equal to four. The MSE generated by this model, however, was quite higher than that of the linear regression model, so it was not the best fitting model for this data. 

```{r warning = FALSE}
fit <- glm(price ~ bs(accommodates, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(accommodates, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(accommodates, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_accomodates <- data.frame(degfree, cv)

#spline on review_scores_rating
fit <- glm(price ~ bs(review_scores_rating, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(review_scores_rating, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(review_scores_rating, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_reviews <- data.frame(degfree, cv)

#spline on bathrooms
fit <- glm(price ~ bs(bathrooms, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(bathrooms, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(bathrooms, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_bathrooms <- data.frame(degfree, cv)

#spline on bedrooms
fit <- glm(price ~ bs(bedrooms, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(bedrooms, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(bedrooms, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_bedrooms <- data.frame(degfree, cv)


df = c(3, 4, 5)
splines = data.frame(df = df, accommodates = df_accomodates$cv, reviews = df_reviews$cv, bedrooms = df_bedrooms$cv, bathrooms = df_bathrooms$cv)
splines = round(splines, 4)
splines
```

|     After fitting the regression splines and obtaining somewhat dismal results, the next step was to fit a General Additive Model (GAM). This model is used for nonlinear relationships with splines on different predictor variables. When forming the GAM, all of the predictor variables in the data were included plus splines on accommodates, review_score_rating, bathrooms, and bedrooms. The splines that were included in this model all used their optimal degrees of freedom found by Cross-Validation MSE values in the last step. 
      This model was definitely more explanatory than the regression splines. The percentage of variation explained by the model was only around 64%, but when fit on the test data, the resulting MSE value was quite comparable to those of the other models fit. The MSE for this model was lower than that of the linear regression model, showing that it was a better fit for the data. 
      Unfortunately, there were a few problems running these models, that would need to be fixed with further analysis. The biggest problem was that there were errors being produced when running the Cross-Validation on the spline models to find optimal degrees of freedom, which is why the range was typically limited to between 3 and 6. In future efforts, this would need to be understood in order to make sure these splines are truly using their optimal degrees of freedom. However, results were still produced, so this would just be a continuation to make our analysis as accurate as possible.

```{r warning = FALSE}
#fit GAM
gammod <- lm(price ~ . + bs(accommodates, df = 2) + bs(review_scores_rating, df = 4) + bs(bathrooms, df = 4) + bs(bedrooms, df = 3), data = training)
summary(gammod)

gam_predict = predict(gammod, newdata = testing)
gam_MSE = round(mean((testing$price - gam_predict)^2), 4)
print(paste("Test MSE of GAM: ", gam_MSE))
```



# PCR and PLS      

|   Moving forward we wanted to try PCR and PLS models. We were curious to see if implementing dimension reduction methods would lead to lower error and better results when predicting log price. Both models were fitted on all input variables just like the regression splines and GAMs. The first model to be fitted was the PCR model. In order to find the optimal number of components 10 fold cross-validation was conducted for models with numbers of components ranging from 1 to 20. Below shows the cross-validation MSE for the corresponding number of components. From this we chose a value of 15 as we deemed the decrease in MSE to be realtively insignificant after 15 components. These 15 components account for approximately 99% of the variability in the data.
```{r,echo = FALSE}
#Fit PCR
m0 <- pcr(price~., data = training,ncomp= 20, validation = "CV")
mse <- MSEP(m0)
data.frame(M = mse$comps, mse = t(as.data.frame(mse$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse)) + ggtitle("Number of Components and MSE")+ xlab("# of components")
# based off of plot we will select 15 components (elbow method) = 99.97% of variance explained
#mse$val
#summary(m0)

# R^2 vs Principal Components Plot
# validationplot(m0, val.type = "R2")


```


|   Below we have our measured versus predicted plot. As you can see in general our predictions due a good job of following the general direction of the red target line with the large moajority of points lying on or around the line. However, as measured values get higher it appears that the PCR model does have a tendency to under predict leading to a fanning out of data points. Our Calculated test set MSE is 0.2192. This outperforms the base regression model which had an MSE of 0.3036.


```{r,echo = FALSE}
# Test Set Predictions
pcr_pred <- predict(m0, testing, ncomp = 15)

# Prediction Plot
predplot(m0, ncomp = 15,line =TRUE, line.col ="red", line.lty ="dashed", main = "PCR Predictions")

# Check for NA values
#sum(is.na(pcr_pred))
#sum(is.na(testing$price))

# Calculate Test Set MSE
PCR_mse <- round(mean((testing$price - pcr_pred)^2), 4)
print(paste("Test MSE of PCR: ", PCR_mse))
```


|   Moving on from PCR we tried out PLS. PLS is very similar to PCR in that they both implement dimension reduction one of the key differences is that PLS is supervised whereas PCR is not. Below we have a plot of the 10-fold cross validation MSE's for different models with number of components ranging from 1 to 20. From this plot we decided to go with 10 components because after 10 components there is almost no significant decrease in MSE. It is interesting to note that the PLS model has 5 less components and still accounts for the same amount of variability in the data at 99%.  

```{r,echo= FALSE}
# Fit PLS
m1 <- plsr(price~., data=training, validation="CV",ncomp=20)

# Plot MSE
mse1 <- MSEP(m1)
data.frame(M = mse1$comps, mse = t(as.data.frame(mse1$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse)) + ggtitle("Number of Components and MSE")+ xlab("# of components")

# Number of Components chosen is 10 = 99.93% of Variance Explained
# summary(m1)


```



|   Below we have plot of the test set predicted versus measured values for the PLS model. Here we can see that in general the predicted/measured points follow the ideal line slightly tighter than the PCR model before. The plot also shows us that the PLS still underpredicts at higher measured values but it appears to be under predicting by less as the data points don't fan out as wide as they do in the PCR. This increase in accuracy is reflected in the MSE with a value of 0.1765 obtained. This is better than both the base regression and the PCR models. 

```{r,echo=FALSE}
# Test Set Predictions
pls_pred <- predict(m1, testing, ncomp = 10)

# Prediction Plot
predplot(m1, ncomp = 10, line = TRUE,line.col ="red", line.lty ="dashed", main = "PLS Predictions")

# Check for NA values
# sum(is.na(pls_pred))

# Calculate Test Set MSE
PLS_mse <- round(mean((testing$price - pls_pred)^2), 4)

print(paste("Test MSE of PLS: ", PLS_mse))
```


# Trees
|   Our next method is decision trees.  Fitting the fit using all predictors, we obtained a tree where five variables were actually used in the tree construction.  Those variables were room_type, longitude, bathrooms, city and bedrooms.  The tree has eight terminal nodes and obtained a test mean squared error of 0.1926.  Because the relationship between the features and the response is approximately linear, linear regression obtains a better MSE than the regression tree.  The cv.tree function is used to perform cross-validation to determine the optimal level of tree complexity.  The optimal level is 8 and then using the prune.tree function, we attempted to prune our tree to the chosen complexity but found that our original tree is the same as the pruned tree.  The table below shows the log prices converted to dollars to help explain the terminal node values.  Node 1 is the farthest node on the left.  

```{r message=FALSE, fig.width=5, fig.height=3,  fig.show='hold'} 
# fit regression tree
initial_tree = tree(price ~ ., data = training)
initial_predict = predict(initial_tree, newdata = testing)
tree_MSE = round(mean((testing$price - initial_predict)^2), 4)
print(paste("Test MSE of Tree: ", tree_MSE))
```
```{r message=FALSE, fig.width=7, fig.height=4, fig.show='hold'} 
plot(initial_tree)
text(initial_tree, cex = 0.55, digits = 4, pretty = 0)
```

```{r message=FALSE, fig.width=5, fig.height=3,  fig.show='hold'} 
node = c(1,2,3,4,5,6,7,8)
log_price = c(4.65, 4.259, 3.753, 4.807, 5.197, 4.905, 5.379, 5.843)
price = round(exp(log_price),2)
tree_explained = data.frame(TerminalNode = node, Log_Price = log_price, Price = price)
kable(tree_explained)
```



# Bagging
|   Because decision trees suffer from high variance, we then moved onto bagging or bootstrap aggregation to see if we could lower the MSE of our trees.  Although bagging decision trees can be slow as it has to average hundreds or thousands of trees together, it won't lead to any overfitting.  We performed bagging on the training set to predict price and found that room type, bathrooms and latitude/longitude where the most important variables.  Our bagging model is able to obtain a MSE of 0.1293 and is found to be one of our best methods. 

```{r}
bag_fit <- randomForest(price ~ ., data = training, mtry = ncol(training) - 1, importance = TRUE)
bag_predict = predict(bag_fit, testing, type = "response")
bag_MSE = round(mean((testing$price - bag_predict)^2), 4)
print(paste("Test MSE of Bagging: ", bag_MSE))
```

```{r echo = FALSE, message=FALSE, fig.show='hold', out.width="50%"}
data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```

|   The plot below shows an Actual vs Predicted plot for the predicted values using bagging.  In general, the data points move positively along the diagonal line with a slight horizontal tilt in the data points compared to the horizontal line.  There are a few outliers especially on the right side of the plot.  It appears bagging is predicting slightly higher than the actual values for prices less than 5 and slightly lower than the actual values for prices more than 5.  However, this plot shows the overall effectiveness for bagging.  


```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3, fig.show='hold'}
ggplot() + 
  geom_point(aes(x = testing$price, y = bag_predict), alpha = 0.25) + 
  geom_abline(slope = 1, intercept = 0) + xlab("Actual") + ylab("Predicted") +
  ggtitle("Performance of Bagging on Test Set")
```



# Random Forests
|   Next, we wanted to determine if random forests could provide an improvement over the bagged trees and see if decorrelating the trees will lead to better MSE result.  The choice of m or the size of the predictor subset is set to the square root of the total predictor set.  Similar to bagging, we didn't have to worry about overfitting and the importance of each variable can be examined.  

|   After performing the random forests with 5000 trees, we found that room type is by far the most important variable with longitude, latitude, bedrooms and accommodates also showing importance.  With a MSE of 0.1301, the overall performance is slightly worse than bagging but still very close.  

```{r}
rf_fit <- randomForest(price ~ ., data = training, mtry = sqrt(ncol(training) - 1), importance = TRUE)
rf_predict = predict(rf_fit, testing, type = "response")
rf_MSE = round(mean((testing$price - rf_predict)^2), 4)
print(paste("Test MSE of Random Forest: ", rf_MSE))
```

```{r echo = FALSE, message=FALSE, fig.show='hold', out.width="50%"}
data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```


|   Similar to bagging, the plot below shows an Actual vs Predicted plot for the predicted values using random forests.  We see the points still move positively along the diagonal line and there are a few outliers.  This plot is very similar to the plot with bagging and because the methods are very similar, this is what we should expect.  

```{r}
ggplot() + 
  geom_point(aes(x = testing$price, y = rf_predict), alpha = 0.25) + 
  geom_abline(slope = 1, intercept = 0) + xlab("Actual") + ylab("Predicted") +
  ggtitle("Performance of Random Forest on Test Set")

```



# Boosting

|   In an attempt to improve our prediction results from our trees, our last method is boosting.  Before boosting, we run a simulation to find the best lambda value and found that the best lambda value to occur around 0.05.  With the number of trees set to 5000, boosting is conducted.  The most important variable was actually property type with bathrooms, room type and bedrooms also relatively important.  It's interesting to see property type with such high importance when bagging and random forests had room type with the largest importance.  The MSE is 0.1317 which is also close to the results for bagging and random forests.  

```{r}
lambdas = seq(0, 0.5, length.out = 100)
trainErrors = rep(NA, length(lambdas))
testErrors = rep(NA, length(lambdas))
for(i in 1:length(lambdas)){
  boost = gbm(price ~ ., data = training, n.trees = 100, shrinkage = lambdas[i], distribution = "gaussian")
  trainPred = predict(boost, training, n.trees = 100)
  testPred = predict(boost, testing, n.trees = 100)
  trainErrors[i] = mean((training$price - trainPred)^2)
  testErrors[i] = mean((testing$price - testPred)^2)
}
```


```{r echo = FALSE, message=FALSE, fig.show='hold', out.width="50%"}
data.frame(x = lambdas, y = trainErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Training MSE") + ggtitle("Shrinkage vs Training MSE")

data.frame(x = lambdas, y = testErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Test MSE") + ggtitle("Shrinkage vs Test MSE")

```

```{r fig.height=5}
boosted = gbm(price ~ ., data = training, n.trees = 5000, shrinkage = lambdas[which.min(testErrors)], distribution = "gaussian")
summary(boosted)

boosted_pred = predict(boosted, testing)
boosted_MSE = round(mean((testing$price - boosted_pred)^2), 4)
print(paste("Testing MSE for Boosted Model:", boosted_MSE))
```



# MSE Results             
```{r}
method = c("Linear Regression", "PCR", "PLS", "Splines", "GAM", "Trees", "Bagging", "Random Forest", "Boosting")
MSEnumbers = c(linearReg_kfold_MSE, PCR_mse, PLS_mse, df_reviews[3,2], gam_MSE, tree_MSE, bag_MSE, rf_MSE, boosted_MSE)

finalMSE = data.frame(Methods = method, MSE = round(MSEnumbers,4), MSE_Dollars = round(exp(MSEnumbers), 2))
kable(finalMSE)
```

|    After implementing all of our methods, we see that bagging and random forests perform the best with boosting not far behind and splines having the worst performance.  To put these errors rate in terms of dollars and not logarithmic dollars, the MSE_Dollars is provided.  As shown, each method has about a dollar of error.  We think that is very good when considering the amount of listings and variables used to train these models.  Given present data, we think this could potentially be useful for current or future AirBnB hosts and AirBnB renters.  


# City Prediction     
|    Our original data set is from a past and completed Kaggle competition.  Thus, the data was split into training and testing files.  The testing file has no response column included because that is how they would determine a winner.  Even though our testing results can't be compared to the actual prices, we can still use it for testing our final model.  

```{r}
final_testing_data = read.csv("testing_data.csv")
testing_df = final_testing_data %>% 
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X)) %>%
  mutate(first_review = as.Date(final_testing_data$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(final_testing_data$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(final_testing_data$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", final_testing_data$host_response_rate))/100)

testing_df = testing_df %>%
  mutate(first_review_year = as.Date(cut(testing_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(testing_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(testing_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

testing_df = testing_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

testing_df = testing_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee)) %>%
  select(-c(first_review, last_review, host_since)) %>%
  mutate(price = 0) 

testing_df = testing_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  filter(property_type != "Earth House")

testing_df = testing_df[complete.cases(testing_df), ]
```

```{r}
levels(testing_df$property_type) <- levels(training_df$property_type)
final_fit <- randomForest(price ~ ., data = training_df, mtry = sqrt(ncol(training_df) - 1), importance = TRUE, ntree = 500)
final_predict = predict(final_fit, testing_df, type = "response")
final_predict
```






\newpage

# References:













