---
title: "Predicting AirBnB Rental Prices"
author: "Group 11: Trevor Isaacson, Jonathan Olavarria, Jasmine DeMeyer"
date: "12/10/2021"
output: beamer_presentation #:
    #theme: "Frankfurt"
    #colortheme: "orchid"
---

```{r setup, include=FALSE, echo = FALSE}
set.seed(445)
# knitr::opts_chunk$set(echo = FALSE)
# knitr::opts_chunk$set(fig.width=5, fig.height=3) 
knitr::opts_chunk$set(warning = FALSE)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(e1071)
library(rmarkdown)
library(glmnet)
library(knitr)
library(leaps)
library(tree)
library(dplyr)
library(caret)
library(gbm)
library(randomForest)
library(GGally)
library(pls)
library(splines)
library(boot)
```

```{r echo = FALSE}
original_df = read.csv("training_data.csv")

training_df = original_df %>% 
  mutate(price = log_price)  %>%
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X, log_price)) %>%
  mutate(first_review = as.Date(original_df$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(original_df$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(original_df$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", original_df$host_response_rate))/100)

training_df = training_df %>%
  mutate(first_review_year = as.Date(cut(training_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(training_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(training_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

training_df = training_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

training_df = training_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee))

training_df = training_df[complete.cases(training_df), ]

training_df <- training_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Island") %>%
  filter(property_type != "Tent") %>%
  filter(property_type != "Treehouse") %>%
  filter(property_type != "Yurt") %>%
  filter(property_type != "Hut") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  select(-c(first_review, last_review, host_since))
```

# Motivation

- You are looking for some additional income and decide renting on AirBnB is the best option
- How much should you rent your extra space for?


# Data

- In general, AirBnB data is very open and be easily accessed
- The original dataset is from a past Kaggle competition 
  - Contained over 74,000 individual listings
- For sake of time and processing power, we took a random sample of 17,500 from those 74,000 listings
- They also provided a testing file
- Since the competition is over, we will compile our final predictions on that file using our best model 

# Data

- Consists of 30 variables
- Variables are about the property, property location, the host and host reviews 
- After cleaning and eliminating variables, our data consisted of 22 variables

```{r, fig.show='hold'}
str(training_df)
```


```{r echo = FALSE}
trn = sample(seq_len(nrow(training_df)), 9000)
training = training_df[trn, ]
testing = training_df[-trn, ]
```

# Baseline Regression

```{r echo = TRUE}
linear = lm(price ~ ., data = training)
```

```{r echo = FALSE}
linear_predict = predict(linear, newdata = testing, type = "response")
MSE_testing = (testing$price - linear_predict)^2
print(paste("MSE of Testing Set: ", round(mean(MSE_testing), 3)))
```

# Regression Splines/Generalized Additive Models

- 20 Fold Cross-Validation was performed for different degrees of freedom ranging usually between 3 and 6
- Cross-Validation MSE used to pick degrees of freedom for splines

# Splines

- Splines fit to variables Accommodates, review_scores_rating, bathrooms, and bedrooms
- Best performing spline based on Cross-Validation MSE was the spline on review_scores_rating with degrees of freedom = 4
- Use these splines with their optimal degrees of freedom in my general additive model

# Spline Degrees of Freedom

```{r}
#spline on accommodates
fit0 <- glm(price ~ bs(accommodates, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(price ~ bs(accommodates, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(accommodates, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(accommodates, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_accomodates <- data.frame(degfree, cv)
df_accomodates

#spline on review_scores_rating
fit <- glm(price ~ bs(review_scores_rating, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(review_scores_rating, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(review_scores_rating, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_reviews <- data.frame(degfree, cv)
df_reviews

#spline on bathrooms
fit <- glm(price ~ bs(bathrooms, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(bathrooms, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(price ~ bs(bathrooms, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]

degfree <- c(3, 4, 5)
cv <- c(cvs, cvs2, cvs3)
df_bathrooms <- data.frame(degfree, cv)
df_bathrooms

#spline on bedrooms
fit <- glm(price ~ bs(bedrooms, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(price ~ bs(bedrooms, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

degfree <- c(3, 4)
cv <- c(cvs, cvs2)
df_bedrooms <- data.frame(degfree, cv)
df_bedrooms
```
# GAM Model

- Performed the GAM on the training data set using all of the predictors plus splines on Accommodates, review_scores_rating, bathrooms, and bedrooms with their optimal degrees of freedom 
- Not a great fitting model, $R^2 = 0.6388
- Decent MSE when fit on the test data set

```{r}
gammod <- lm(price ~ . + bs(accommodates, df = 2) + bs(review_scores_rating, df = 4) + bs(bathrooms, df = 4) + bs(bedrooms, df = 3), data = training)

gam_predict = predict(gammod, newdata = testing)
gam_MSE = round(mean((testing$price - gam_predict)^2), 4)
print(paste("Test MSE of GAM: ", gam_MSE))
```

# Future Modeling with Splines

- Received errors when using degrees of freedom larger than 6 or so
- Want to look into these errors and figure out if I could try larger degrees of freedom in my splines to get a better model

# PCR and PLS

- 10 Fold Cross-Validation was performed for number of components ranging from 1 to 20.
- The Cross-Validation MSE was used to pick optimal number of components for both models.

# PCR

```{r, echo = FALSE}
#Fit PCR
m0 <- pcr(price~., data = training,ncomp= 20, validation = "CV")
mse <- MSEP(m0)
data.frame(M = mse$comps, mse = t(as.data.frame(mse$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse)) + ggtitle("Principal Component Regression MSE") +xlab("Number of Components")
```

# PCR Predictions

```{r, echo = FALSE}
predplot(m0, ncomp = 15, main ="PCR Predictions", line = TRUE,line.col="red")
```


# PLS

```{r,echo = FALSE}
m1 <- plsr(price~., data=training, validation="CV",ncomp=20)

# Plot MSE
mse1 <- MSEP(m1)
data.frame(M = mse1$comps, mse = t(as.data.frame(mse1$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse)) + ggtitle("Partial Least Squares MSE") + xlab("Number of Components")

```


# PLS Predictions

```{r, echo = FALSE}
pls_pred <- predict(m1, testing, ncomp = 10)
predplot(m1, ncomp = 10, main ="PLS Predictions", line = TRUE,line.col="red")
```


# PCR and PLS Summary

```{r,echo=FALSE}

# PCR Test Set Predictions
pcr_pred <- predict(m0, testing, ncomp = 15)

# PCR Calculate Test Set MSE
PCR_mse <- round(mean((testing$price - pcr_pred)^2), 4)


# PLS Test Set Predictions
pls_pred <- predict(m1, testing, ncomp = 10)

# PLS Calculate Test Set MSE
PLS_mse <- round(mean((testing$price - pls_pred)^2), 4)


tab <- matrix(c(15,PLS_mse,99.7,10,PCR_mse,99.9 ),ncol =2)
colnames(tab) <- c("PCR","PLS")
rownames(tab) <- c("Components", "Test MSE", "% Variance Explained")
tab <- as.table(tab)
tab
```



# Regression Trees
```{r echo = FALSE, fig.height=7, fig.width=5} 
# fit regression tree
initial_tree = tree(price ~ ., data = training)
summary(initial_tree)

initial_predict = predict(initial_tree, newdata = testing)
tree_MSE = round(mean((testing$price - initial_predict)^2), 4)
print(paste("Test MSE of Initial Tree: ", tree_MSE))
```

# Regression Trees
```{r echo = FALSE}
# cross validation
initial_cv = cv.tree(initial_tree)
ggplot() +
  geom_point(aes(x = initial_cv$size, y = initial_cv$dev)) + 
  geom_line(aes(x = initial_cv$size, y = initial_cv$dev)) + 
  ylab("CV Error Rate") + xlab("Size") + ggtitle("CV Classification Error Rate")
```

# Regression Trees
```{r echo = FALSE, fig.show='hold', fig.height = 7}
plot(initial_tree)
text(initial_tree, cex = 1, digits = 4, pretty = 0)
```

# Bagging

```{r echo = TRUE}
bag_fit <- randomForest(price ~ ., data = training, mtry = ncol(training) - 1, importance = TRUE)
bag_predict = predict(bag_fit, testing, type = "response")
bag_MSE = round(mean((testing$price - bag_predict)^2), 4)
print(paste("Test MSE of Bagging: ", bag_MSE))
```

# Bagging

```{r, echo = FALSE, fig.show='hold', fig.height = 7, out.width="50%"}
data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```

# Random Forests

```{r echo = TRUE}
rf_fit <- randomForest(price ~ ., data = training, mtry = sqrt(ncol(training) - 1), importance = TRUE)
```

```{r echo = FALSE}
rf_predict = predict(rf_fit, testing, type = "response")
rf_MSE = round(mean((testing$price - rf_predict)^2), 4)
print(paste("Test MSE of Random Forest: ", rf_MSE))
```

# Random Forests

```{r, echo = FALSE, fig.show='hold', fig.height = 7, out.width="50%"}
data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("% Decreasing MSE of Random Forest") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Node Purity of Random Forest") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```

# Boosting

```{r echo = FALSE, fig.show='hold', fig.height = 7}
lambdas = seq(0, 0.5, length.out = 100)
trainErrors = rep(NA, length(lambdas))
testErrors = rep(NA, length(lambdas))
for(i in 1:length(lambdas)){
  boost = gbm(price ~ ., data = training, n.trees = 1000, shrinkage = lambdas[i], distribution = "gaussian")
  trainPred = predict(boost, training, n.trees = 1000)
  testPred = predict(boost, testing, n.trees = 1000)
  trainErrors[i] = mean((training$price - trainPred)^2)
  testErrors[i] = mean((testing$price - testPred)^2)
}

data.frame(x = lambdas, y = trainErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Training MSE") + ggtitle("Shrinkage vs Training MSE")
```

# Boosting 

```{r echo = FALSE}
boosted = gbm(price ~ ., data = training, n.trees = 5000, shrinkage = lambdas[which.min(testErrors)], distribution = "gaussian")
summary(boosted)

boosted_pred = predict(boosted, testing)
boosted_MSE = round(mean((testing$price - boosted_pred)^2), 4)
print(paste("Testing MSE for Boosted Model:", boosted_MSE))
```

# MSE Table
```{r echo = FALSE}
method = c("Linear Regression", "PCR", "PLS", "Splines", "GAM", "Trees", "Bagging", "Random Forest", "Boosting")
MSEnumbers = c(linearReg_kfold_MSE, PCR_mse, PLS_mse, df_reviews[3,2], gam_MSE, tree_MSE, bag_MSE, rf_MSE, boosted_MSE)

finalMSE = data.frame(Methods = method, MSE = round(MSEnumbers,4), MSE_Dollars = round(exp(MSEnumbers), 2))
finalMSE
```

# Final Model
```{r echo = FALSE}
final_testing_data = read.csv("testing_data.csv")
testing_df = final_testing_data %>% 
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X)) %>%
  mutate(first_review = as.Date(final_testing_data$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(final_testing_data$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(final_testing_data$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", final_testing_data$host_response_rate))/100)

testing_df = testing_df %>%
  mutate(first_review_year = as.Date(cut(testing_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(testing_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(testing_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

testing_df = testing_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

testing_df = testing_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee)) %>%
  select(-c(first_review, last_review, host_since)) %>%
  mutate(price = 0) 

testing_df = testing_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  filter(property_type != "Earth House")

testing_df = testing_df[complete.cases(testing_df), ]

```

```{r echo = FALSE}
levels(testing_df$property_type) <- levels(training_df$property_type)
final_fit <- randomForest(price ~ ., data = training_df, mtry = sqrt(ncol(training_df) - 1), importance = TRUE, ntrees = 200)
final_predict = predict(final_fit, testing_df, type = "response")
```

```{r}
head(final_predict)
length(final_predict)
nrow(testing_df)
testing_df$price = final_predict

```



# Questions?




## References

