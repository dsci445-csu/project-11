---
title: "Predicting AirBnB Rental Prices"
author: "Group 11: Trevor Isaacson, Jonathan Olavarria, Jasmine DeMeyer"
date: "12/10/2021"
output: beamer_presentation #:
    #theme: "Frankfurt"
    #colortheme: "orchid"
---

```{r setup, include=FALSE, echo = FALSE}
set.seed(445)
# knitr::opts_chunk$set(echo = FALSE)
# knitr::opts_chunk$set(fig.width=5, fig.height=3) 
knitr::opts_chunk$set(warning = FALSE)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(e1071)
library(rmarkdown)
library(glmnet)
library(knitr)
library(leaps)
library(tree)
library(dplyr)
library(caret)
library(gbm)
library(randomForest)
library(GGally)
library(pls)
library(splines)
library(boot)
```

```{r echo = FALSE}
original_df = read.csv("training_data.csv")

training_df = original_df %>% 
  mutate(price = log_price)  %>%
  select(-c(id, amenities, description, thumbnail_url, zipcode, name, neighbourhood, X, log_price)) %>%
  mutate(first_review = as.Date(original_df$first_review, format = "%Y-%m-%d")) %>%
  mutate(last_review = as.Date(original_df$last_review, format = "%Y-%m-%d")) %>%
  mutate(host_since = as.Date(original_df$host_since, format = "%Y-%m-%d")) %>%
  mutate(host_response_rate = as.numeric(sub("%", "", original_df$host_response_rate))/100)

training_df = training_df %>%
  mutate(first_review_year = as.Date(cut(training_df$first_review, "year"))) %>%
  mutate(last_review_year = as.Date(cut(training_df$last_review, "year"))) %>%
  mutate(host_since_year = as.Date(cut(training_df$host_since, "year"))) %>%
  mutate(host_has_profile_pic = replace(host_has_profile_pic, host_has_profile_pic == "", "f")) %>%   # eliminate blank values
  mutate(host_identity_verified = replace(host_identity_verified, host_identity_verified == "", "f"))  # eliminate blank values

training_df = training_df %>%
  mutate(first_review_year = as.numeric(format(first_review_year, format = "%Y"))) %>%
  mutate(last_review_year = as.numeric(format(last_review_year, format = "%Y"))) %>%
  mutate(host_since_year = as.numeric(format(host_since_year, format = "%Y")))

training_df = training_df %>%
  mutate(first_review_year = factor(ifelse(first_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(last_review_year = factor(ifelse(last_review_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_since_year = factor(ifelse(host_since_year <= 2014, "Less2014", "Greater2014"))) %>%
  mutate(host_response_rate  = if_else(is.na(host_response_rate), 0, host_response_rate)) %>%
  mutate(cleaning_fee = as.factor(cleaning_fee))

training_df = training_df[complete.cases(training_df), ]

training_df <- training_df %>%
  filter(property_type != "Chalet") %>%
  filter(property_type != "Island") %>%
  filter(property_type != "Tent") %>%
  filter(property_type != "Treehouse") %>%
  filter(property_type != "Yurt") %>%
  filter(property_type != "Hut") %>%
  filter(property_type != "Train") %>%
  filter(property_type != "Vacation home") %>%
  select(-c(first_review, last_review, host_since))
```

# Motivation

- You are looking for some additional income and decide renting on AirBnB is the best option
- How much should you rent your extra space for?


# Data

- In general, AirBnB data is very open and be easily accessed
- The original dataset is from a past Kaggle competition 
  - Contained over 74,000 individual listings
- For sake of time and processing power, we took a random sample of 17,500 from those 74,000 listings
- They also provided a testing file
- Since the competition is over, we will compile our final predictions on that file using our best model 

# Data

- Original data consists of 30 variables
- Variables are about the property, property location, the host and host reviews 
- After cleaning and eliminating variables, our data consisted of 22 variables
- Property:
  - property_type, room_type, accommodates, bedrooms, beds, bed_type, bathrooms
- Location: 
  - latitude, longitude, city
- Host:
  - cancellation_policy, cleaning_fee, host_has_profile_pic, host_identify_verified, etc


```{r echo = FALSE}
trn = sample(seq_len(nrow(training_df)), 9000)
training = training_df[trn, ]
testing = training_df[-trn, ]
```

# Baseline Regression

```{r echo = TRUE}
linear = lm(price ~ ., data = training)
```

```{r echo = FALSE}
linear_predict = predict(linear, newdata = testing, type = "response")
MSE_testing = (testing$price - linear_predict)^2
print(paste("MSE of Testing Set: ", round(mean(MSE_testing), 3)))
```

# Regression Splines/Generalized Additive Models





# PCR and PLS

- 10 Fold Cross-Validation was performed for number of components ranging from 1 to 20.
- The Cross-Validation MSE was used to pick optimal number of components for both models.

# PCR

```{r, echo = FALSE}
#Fit PCR
m0 <- pcr(price~., data = training,ncomp= 20, validation = "CV")
mse <- MSEP(m0)
data.frame(M = mse$comps, mse = t(as.data.frame(mse$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse)) + ggtitle("Principal Component Regression MSE") +xlab("Number of Components")
```

# PCR Predictions

```{r, echo = FALSE}
predplot(m0, ncomp = 15, main ="PCR Predictions", line = TRUE,line.col="red")
```


# PLS

```{r,echo = FALSE}
m1 <- plsr(price~., data=training, validation="CV",ncomp=20)

# Plot MSE
mse1 <- MSEP(m1)
data.frame(M = mse1$comps, mse = t(as.data.frame(mse1$val))[, "CV"]) %>%
ggplot() +
  geom_line(aes(M, mse)) +
  geom_point(aes(M, mse)) + ggtitle("Partial Least Squares MSE") + xlab("Number of Components")

```


# PLS Predictions

```{r, echo = FALSE}
pls_pred <- predict(m1, testing, ncomp = 10)
predplot(m1, ncomp = 10, main ="PLS Predictions", line = TRUE,line.col="red")
```


# PCR and PLS Summary

```{r,echo=FALSE}

# PCR Test Set Predictions
pcr_pred <- predict(m0, testing, ncomp = 15)

# PCR Calculate Test Set MSE
PCR_mse <- round(mean((testing$price - pcr_pred)^2), 4)


# PLS Test Set Predictions
pls_pred <- predict(m1, testing, ncomp = 10)

# PLS Calculate Test Set MSE
PLS_mse <- round(mean((testing$price - pls_pred)^2), 4)


tab <- matrix(c(15,PLS_mse,99.7,10,PCR_mse,99.9 ),ncol =2)
colnames(tab) <- c("PCR","PLS")
rownames(tab) <- c("Components", "Test MSE", "% Variance Explained")
tab <- as.table(tab)
tab
```



# Regression Trees
```{r echo = FALSE, fig.height=7, fig.width=5} 
# fit regression tree
initial_tree = tree(price ~ ., data = training)
summary(initial_tree)

initial_predict = predict(initial_tree, newdata = testing)
tree_MSE = round(mean((testing$price - initial_predict)^2), 4)
print(paste("Test MSE of Initial Tree: ", tree_MSE))
```

# Regression Trees
```{r echo = FALSE}
# cross validation
initial_cv = cv.tree(initial_tree)
ggplot() +
  geom_point(aes(x = initial_cv$size, y = initial_cv$dev)) + 
  geom_line(aes(x = initial_cv$size, y = initial_cv$dev)) + 
  ylab("CV Error Rate") + xlab("Size") + ggtitle("CV Classification Error Rate")
```

# Regression Trees
```{r echo = FALSE, fig.show='hold', fig.height = 7}
plot(initial_tree)
text(initial_tree, cex = 1, digits = 4, pretty = 0)
```

# Bagging

```{r echo = TRUE}
bag_fit <- randomForest(price ~ ., data = training, mtry = ncol(training) - 1, importance = TRUE, ntrees = 2500)
bag_predict = predict(bag_fit, testing, type = "response")
bag_MSE = round(mean((testing$price - bag_predict)^2), 4)
print(paste("Test MSE of Bagging: ", bag_MSE))
```

# Bagging

```{r, echo = FALSE, fig.show='hold', fig.height = 7, out.width="50%"}
data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(bag_fit$importance) %>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Predictive Power") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```

# Random Forests

```{r echo = TRUE}
rf_fit <- randomForest(price ~ ., data = training, mtry = sqrt(ncol(training) - 1), importance = TRUE, ntrees = 2500)
```

```{r echo = FALSE}
rf_predict = predict(rf_fit, testing, type = "response")
rf_MSE = round(mean((testing$price - rf_predict)^2), 4)
print(paste("Test MSE of Random Forest: ", rf_MSE))
```

# Random Forests

```{r, echo = FALSE, fig.show='hold', fig.height = 7, out.width="50%"}
data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(X.IncMSE)])) %>%
  ggplot() + ggtitle("% Decreasing MSE of Random Forest") + xlab("% Decreasing MSE") + 
  geom_point(aes(X.IncMSE, variable))  

data.frame(rf_fit$importance) %>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(IncNodePurity)])) %>%
  ggplot() + ggtitle("Node Purity of Random Forest") + xlab("Node Purity") + 
  geom_point(aes(IncNodePurity, variable))  

```

# Boosting

```{r echo = FALSE, fig.show='hold', fig.height = 7}
lambdas = seq(0, 0.5, length.out = 75)
trainErrors = rep(NA, length(lambdas))
testErrors = rep(NA, length(lambdas))
for(i in 1:length(lambdas)){
  boost = gbm(price ~ ., data = training, n.trees = 500, shrinkage = lambdas[i], distribution = "gaussian")
  trainPred = predict(boost, training, n.trees = 500)
  testPred = predict(boost, testing, n.trees = 500)
  trainErrors[i] = mean((training$price - trainPred)^2)
  testErrors[i] = mean((testing$price - testPred)^2)
}

data.frame(x = lambdas, y = trainErrors) %>%
  ggplot(aes(x = x , y = y)) + geom_point() +
  xlab("Shrinkage") + ylab("Training MSE") + ggtitle("Shrinkage vs Training MSE")
```

# Boosting 

```{r echo = FALSE, warning = FALSE}
boosted = gbm(price ~ ., data = training, n.trees = 2500, shrinkage = lambdas[which.min(testErrors)], distribution = "gaussian")
```

```{r echo = FALSE,  fig.show='hold', fig.height = 7}
boosted_pred = predict(boosted, testing)
boosted_MSE = round(mean((testing$price - boosted_pred)^2), 4)
print(paste("Testing MSE for Boosted Model:", boosted_MSE))
print(paste("Best Variables: property_type, room_type, bathrooms"))
summary(boosted)
```

# MSE Table
```{r echo = FALSE}
method = c("Linear Regression", "PCR", "PLS", "Splines", "GAM", "Trees", "Bagging", "Random Forest", "Boosting")
# MSEnumbers = c(MSE_testing, PCR_mse, PLS_mse, df_reviews[3,2], gam_MSE, tree_MSE, bag_MSE, rf_MSE, boosted_MSE)

# finalMSE = data.frame(Methods = method, MSE = round(MSEnumbers,4), MSE_Dollars = round(exp(MSEnumbers), 2))
# finalMSE
```


# Going Forward

- Our data has data from multiple cities across the country
- Can we apply this to a certain city and see similar results?
- Is this accurate enough to help AirBnB hosts in selected cities?
  - Using current data, can this model help hosts correctly adjust their rates?

# Questions?




## References

